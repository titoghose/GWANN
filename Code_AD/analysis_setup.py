import os
import sys

import tqdm

sys.path.append('/home/upamanyu/GWANN')

import multiprocessing as mp
import shutil
import warnings
from functools import partial
from typing import Optional
import pickle
import cuml
import yaml

warnings.filterwarnings('ignore')

import seaborn as sns
import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
import scipy.stats as stats
from sklearn.model_selection import StratifiedShuffleSplit

from GWANN.dataset_utils import (balance_by_agesex, create_groups, group_ages,
                                 find_num_wins, PGEN2Pandas, genomic_region_PCA,
                                 genomic_PCA)
from GWANN.utils import vprint


def variant_gene_mapping(chrom:str, annotation_base:str) -> None:
    """Find the nearest gene to each variant in the UKBB data. This is
    done by using the ANNOVAR output files.
    
    Parameters
    ----------
    chrom : str
        Chromosome number for which the variant-gene mapping is to be
        done.
    annotation_base : str
        Path to the folder containing the ANNOVAR output files.

    Returns
    -------
        None
    """

    def nearest_gene(genes:str) -> str:
        """Find the nearest gene to the variant based on the distance"""
        genes = genes.replace(';', ',')
        glist = genes.split(',')
        gdict = {}
        for g in glist:
            
            symbol = g.split('(')[0]
            if "dist=" not in g:
                dist = 0
            else:    
                dist = g.split('(')[1].replace('dist=','').replace(')','')
                try: 
                    dist=int(dist) 
                except ValueError: 
                    dist=1e12
            gdict[symbol] = dist
        return min(gdict, key=gdict.get)

    def gene_snp_count() -> None:
        """From the file generated by variant_gene_mapping, find the number
        of variants mapped to each gene."""
        
        variant_annot = pd.read_csv(f'{annotation_base}/UKB_chr{chrom}.variant_gene_mapping.csv')
        variant_annot['mapped_gene'].value_counts().to_csv(
            f'{annotation_base}/UKB_chr{chrom}.gene_snp_count.csv')

    variant_annot = pd.read_csv(f'{annotation_base}/UKB_chr{chrom}.variant_function', 
                                sep='\t', header=None)
    variant_annot.rename(columns={0:'annot', 1:'genes', 2:'chrom', 3:'start', 
                                  4:'end', 5:'ref', 6:'alt', 7:'snpid'}, 
                        inplace=True)
    variant_annot['mapped_gene'] = variant_annot['genes'].apply(
                                        lambda x: nearest_gene(x))
    variant_annot.to_csv(f'{annotation_base}/UKB_chr{chrom}.variant_gene_mapping.csv', 
                         index=False)
    gene_snp_count()

def filter_ids(param_folder:str, phen_cov_path:str) -> None:
    """Additional filters to remove ids that were selected during the
    original analysis to create a new set of ids for the re-run to
    address reviewer comments. Retain most of the old ids except where
    (i) individuals withdrew participation from UKB, (ii) have any
    missing values for covariates, or (iii) sample is a control but is
    in the list of AD diagnosed people.

    param_folder : str
        Path to the folder containing experiment parameters or the
        folder where all additional parameter files should be saved.
    phen_cov_path : str
        Path to the file containing the covariates and phenotype
        information for all individuals in the cohort.
    """
    extended_AD_ids = pd.read_csv(
        './params/UKB_AD_inc_c.sample', sep='\t').iloc[1:,:]['ID_1'].to_list()
    
    ukb_withdrawn_ids = pd.read_csv(
            'params/ukb_withdrawn_04May23.csv')['ID_1'].to_list()

    neuro_ids = pd.read_csv('./params/Neuro_Diagnosis.csv')['ID_1'].to_list()

    with open(f'{param_folder}/covs_MATERNAL_MARIONI.yaml', 'r') as f:
        covs = yaml.load(f, yaml.FullLoader)['COVARIATES']
    
    phen_cov_df = pd.read_csv(phen_cov_path, sep=' ', comment='#')
    phen_cov_df = phen_cov_df[
        ['ID_1',] + covs + ['MATERNAL_MARIONI', 'PATERNAL_MARIONI']]
    phen_cov_df.set_index('ID_1', inplace=True, drop=False)
    phen_cov_df.dropna(subset=covs, axis=0, how='any', inplace=True)
    
    original_ids = []
    for phen in ['MATERNAL_MARIONI', 'PATERNAL_MARIONI']:
        for split in ['train', 'test']:
            ids = pd.read_csv(f'./params/original_run/{split}_ids_{phen}.csv')['iid'].to_list()
            original_ids.extend(ids)
    
    add_df = phen_cov_df.loc[~phen_cov_df.index.isin(original_ids)]
    add_df = add_df.loc[
                (add_df['MATERNAL_MARIONI'] != 1) & 
                (add_df['PATERNAL_MARIONI'] != 1) &
                (~add_df.index.isin(neuro_ids)) &
                (~add_df.index.isin(extended_AD_ids)) &
                (~add_df.index.isin(ukb_withdrawn_ids))]

    for phen in ['MATERNAL_MARIONI', 'PATERNAL_MARIONI']:
        for split in ['train', 'test']:
            
            vprint(f'\n\n{phen} - {split}')
            vprint('------------')
            id_df = pd.read_csv(f'./params/original_run/{split}_ids_{phen}.csv')
            id_df.set_index('iid', inplace=True, drop=False)
            vprint(id_df.groupby(phen).count().T)

            # Remove iids with missing covariate data   
            id_df = id_df.loc[id_df.index.isin(phen_cov_df.index)]
            
            # Remove controls with AD diagnosis
            id_df = id_df.loc[~((id_df[phen] == 0) & 
                                (id_df.index.isin(extended_AD_ids)))]
            
            # Remove cases and controls in the withdrawn list
            id_df = id_df.loc[~id_df.index.isin(ukb_withdrawn_ids)]
            
            vprint(f'\nAfter filters:')
            vprint(id_df.groupby(phen).count().T)
            # print(id_df.groupby(phen).count().index)

            # If n(controls) < n(cases) add controls to make ratio 1:1
            num_extra_cases = (id_df.loc[id_df[phen] == 1].shape[0] - 
                               id_df.loc[id_df[phen] == 0].shape[0])
            add_controls = add_df.loc[add_df[phen] == 0].sample(
                                        num_extra_cases, random_state=2617)
            add_df.drop(index=add_controls.index, inplace=True)
            add_controls.rename(columns={'ID_1': 'iid'}, inplace=True)
            id_df = pd.concat((id_df, add_controls[['iid', phen]]))
    
            vprint(f'\nAfter adding new controls:')
            vprint(id_df.groupby(phen).count().T)

            id_df.to_csv(f'./params/reviewer_rerun/{split}_ids_{phen}.csv', 
                         index=False)
        
        create_groups(
            label=phen,
            param_folder=param_folder, 
            phen_cov_path=phen_cov_path,
            grp_size=10
        )

def find_all_ids(param_folder:str, phen_cov_path:str, control_prop:float=1.0) -> None:
    """From all possible indidividuals in the UKBB data, generate 1:1 
    case:control split and save all ids to a file. Do this for Maternal
    and Paternal histories.

    Parameters
    ----------
    param_folder : str
        Path to the folder containing experiment parameters or the
        folder where all additional parameter files should be saved.
    phen_cov_path : str
        Path to the file containing the covariates and phenotype
        information for all individuals in the cohort.
    """
    
    with open(f'{param_folder}/covs_FH_AD.yaml', 'r') as f:
        covs = yaml.load(f, yaml.FullLoader)['COVARIATES']

    geno_id_path = 'params/geno_ids.csv'.format(param_folder)
    geno_ids = pd.read_csv(geno_id_path)['ID1'].values

    ad_diag_path = 'params/AD_Diagnosis.csv'.format(param_folder)
    ad_diag = pd.read_csv(ad_diag_path)['ID1'].to_list()
    extended_AD_ids = pd.read_csv(
        './params/UKB_AD_inc_c.sample', sep='\t').iloc[1:,:]
    extended_AD_ids = extended_AD_ids.loc[extended_AD_ids['phenotype']==1]['ID_1'].to_list()
    ad_diag = ad_diag + list(set(extended_AD_ids))
    print(f'Num of AD diagnosed: {len(ad_diag)}')

    neuro_diag_path = 'params/Neuro_Diagnosis.csv'.format(param_folder)
    neuro_diag = pd.read_csv(neuro_diag_path)['ID_1'].values
    
    ukb_withdrawn_ids = pd.read_csv(
            'params/ukb_withdrawn_04May23.csv')['ID_1'].to_list()

    df = pd.read_csv(phen_cov_path, sep=' ', comment='#')
    df.drop_duplicates('ID_1', inplace=True)
    df.set_index('ID_1', drop=False, inplace=True)
    df = df.loc[df.index.isin(geno_ids)]
    df = df.loc[~df.index.isin(ukb_withdrawn_ids)]
    print('Shape after retaining only those iids in genotype file: {}'.format(df.shape))

    # Remove people with AD diagnosis but in the FH control set and add
    # them to the maternal and paternal cases
    df.loc[(df['MATERNAL_MARIONI'] == 1) | (df['PATERNAL_MARIONI'] == 1), 'FH_AD'] = 1
    df.loc[(df['MATERNAL_MARIONI'] == 0) & (df['PATERNAL_MARIONI'] == 0), 'FH_AD'] = 0
    
    df.loc[df.index.isin(ad_diag), 'FH_AD'] = 1
    
    old_len = df.shape[0]
    df = df.loc[~(df.index.isin(neuro_diag) & (df['FH_AD']==0))]
    new_len = df.shape[0]
    print('Number of Neuro diagnosed removed from controls: {}'.format(old_len-new_len))

    df.dropna(subset=covs, inplace=True)
    print('Shape after dropping missing covariates: {}'.format(df.shape))
    
    # Bin ages to enable balanecd sampling between cases and controls
    new_ages = group_ages(df['f.21003.0.0'].values, 10)
    df['old_ages'] = df['f.21003.0.0'].values
    df['f.21003.0.0'] = new_ages
    print('Number of unique age groups: {}'.format(
        np.unique(df['f.21003.0.0'].values)))

    # Ensure that MATERNAL_MARIONI dataset is created first
    label = 'FH_AD'
    print('\nTrain-test split for: {}'.format(label))
    all_ids_path = '{}/all_ids_{}.csv'.format(param_folder, label)
    train_ids_path = '{}/train_ids_{}.csv'.format(param_folder, label)
    test_ids_path = '{}/test_ids_{}.csv'.format(param_folder, label)
    
    lab_df = df.copy()
    lab_df = lab_df.loc[lab_df[label].isin([0, 1])]

    # Get controls balanced by age and sex
    b_df = balance_by_agesex(lab_df, label, control_prop=control_prop)
    print('Final df size, cases, controls: {} {} {} {}'.format(
        b_df.shape[0], 
        b_df.loc[b_df[label] == 1].shape[0],
        b_df.loc[b_df[label] == 0].shape[0],
        b_df.loc[b_df[label].isna()].shape[0]))
    
    b_df = b_df.rename(columns={'ID_1':'iid'})
    b_df[['iid', label]].to_csv(all_ids_path, index=False)
    
    sss = StratifiedShuffleSplit(1, test_size=0.10, random_state=0)
    for tr, te in sss.split(b_df.values, b_df[label].values):
        test_df = b_df.iloc[te]
        train_df = b_df.iloc[tr]
    
    print('Final train_df size, cases, controls: {} {} {}'.format(
        train_df.shape[0], 
        train_df.loc[train_df[label] == 1].shape[0],
        train_df.loc[train_df[label] == 0].shape[0]))

    print('Final test_df size, cases, controls: {} {} {}'.format(
        test_df.shape[0], 
        test_df.loc[test_df[label] == 1].shape[0],
        test_df.loc[test_df[label] == 0].shape[0]))

    test_df = test_df.rename(columns={'ID_1':'iid'})
    test_df[['iid', label]].to_csv(test_ids_path, index=False)
    
    train_df = train_df.rename(columns={'ID_1':'iid'})
    train_df[['iid', label]].to_csv(train_ids_path, index=False)
    
    for k, v in {'ages':'old_ages', 'sex':'f.31.0.0'}.items():
        D, p = stats.ks_2samp(train_df[v], test_df[v])
        print(f"Test and train {k} KS test: p={p}")
        
        D, p = stats.ks_2samp(train_df.loc[train_df[label]==0][v], 
                                train_df.loc[train_df[label]==1][v])
        print(f"Train case vs control {k} KS test: p={p}")
        
        D, p = stats.ks_2samp(test_df.loc[test_df[label]==0][v], 
                                test_df.loc[test_df[label]==1][v])
        print(f"Test case vs control {k} KS test: p={p}")
        
        train_df['type'] = 'train'
        test_df['type'] = 'test'
        comb_df = pd.concat((train_df, test_df))
        g = sns.displot(data=comb_df, x=v, col='type', hue=label, 
                    kind='hist', common_bins=True, common_norm=False, 
                    stat='density', alpha=0.5)
        g.add_legend()
        plt.savefig(f'{param_folder}/{k}_dist_{label}.png')
        plt.close()

def num_wins_per_gene():
    gdf = pd.read_csv('/home/upamanyu/GWANN/GWANN/datatables/gene_annot.csv', 
                      dtype={'chrom':str})
    gdf['num_wins'] = 0
    for chrom, idxs in gdf.groupby('chrom').groups.items():
        if int(chrom)%2 != 0:
            continue
        cdf = gdf.loc[idxs].head(100)
        pvar = pd.read_csv(f'/mnt/sdf/GWANN_pgen/UKB_chr{chrom}.pvar', 
                           sep='\t', dtype={'#CHROM':str})
        pvar.rename(columns={'#CHROM':'CHROM'}, inplace=True)

        fargs = list(cdf[['chrom', 'start', 'end']].itertuples(index=False, name=None))
        wins_cnt_func = partial(find_num_wins, pgen_data=pvar.loc[pvar['CHROM'] == chrom], 
                                win_size=50)
        with mp.Pool(20) as pool:
            cnts = pool.starmap(wins_cnt_func, fargs, chunksize=1)
            pool.close()
            pool.join()
        cdf['num_wins'] = cnts
        print(cdf.head())
        print()
    
def dosage_percentage():
    base = '/home/upamanyu/GWANN_data/Data_MatAD/wins'
    flist = os.listdir(base)
    dos_gt_ptg = []
    for f in tqdm.tqdm(flist[:500]):
        df = pd.read_csv(f'{base}/{f}', index_col=0)
        df = df[df.columns[:-10]]
        hard_gt = np.count_nonzero(np.isin(df.values, [0.0, 1.0, 2.0]))
        total_gt = df.shape[0]*df.shape[1]
        dos_gt_ptg.append((total_gt-hard_gt)/total_gt)
        # break
    
    sns.histplot(x=dos_gt_ptg)
    plt.savefig('dosage_percentage_dist.png', dpi=100)
    plt.close()

def gene_PCs():
    # variant_annot = pd.read_csv('/mnt/sdf/annovar_output/UKB_chr2.variant_gene_mapping.csv')

    # variant_annot['start'] = variant_annot['start'].astype(int)
    # variant_annot['end'] = variant_annot['end'].astype(int)
    # variant_annot['start'] = variant_annot.groupby('mapped_gene')['start'].transform('min')
    # variant_annot['end'] = variant_annot.groupby('mapped_gene')['end'].transform('max')
    # variant_annot['snp_cnt'] = variant_annot.groupby('mapped_gene')['snpid'].transform('count')
    # variant_annot.drop_duplicates('mapped_gene', inplace=True)
    # variant_annot.sort_values('snp_cnt', ascending=False, inplace=True)

    # variant_annot = variant_annot.sample(1000)

    # fargs = []
    # pgen2pd = '/mnt/sdf/GWANN_pgen/UKB_chr2'
    # for _, row in variant_annot.iterrows():
    #     fargs.append((pgen2pd, '2', row['start'], row['end'], 0.7, 20_000))

    # # res = []
    # # for farg in fargs:
    # #     res.append(genomic_region_PCA(*farg))
    
    # with mp.Pool(20) as pool:
    #     res = pool.starmap(genomic_region_PCA, fargs, chunksize=1)
    #     pool.close()
    #     pool.join()

    # df = pd.DataFrame.from_records(res, columns=['EVR', 'num_PCs', 'num_SNPs'])
    # df.to_csv('chrom2_gene_PCs.csv', index=False)

    chrom = os.environ['CHROM']
    os.makedirs(f'/mnt/sdf/GWANN_PCA_models/{chrom}', exist_ok=True)

    pgen2pd = f'/mnt/sdf/GWANN_pgen/UKB_chr{chrom}'
    train_ids_f = '/home/upamanyu/GWANN/Code_AD/params/reviewer_rerun_Sens8/train_ids_FH_AD.csv'
    train_ids_df = pd.read_csv(train_ids_f, dtype={'iid':str})
    train_ids = train_ids_df['iid'].to_list()

    pca_list = genomic_PCA(chrom, pgen2pd, train_ids, 0.7, 
                           start_num_snps=20_000, 
                           step=100, num_PCs=50)
    
    # Loop through and save each PCA model separately
    fnames = []
    for d in pca_list:
        pca = d['pca']
        
        chrom = d['chrom']
        start = d['start']
        end = d['end']
        evr = d['evr']
        
        fname = f"pca_{chrom}_{start}_{end}.pkl"
        fnames.append(f'{fname}\t{evr}')
        
        with open(f'/mnt/sdf/GWANN_PCA_models/{chrom}/{fname}', 'wb') as f:
            pickle.dump(pca, f) 

    with open(f'/mnt/sdf/GWANN_PCA_models/{chrom}/metadata.txt', 'w') as f:
        f.write('\n'.join(fnames))

if __name__ == '__main__':
    
    # 1. Find all train and test ids
    # find_all_ids(
    #     param_folder='/home/upamanyu/GWANN/Code_AD/params/reviewer_rerun_Sens7', 
    #     phen_cov_path='/mnt/sdg/UKB/Variables_UKB.txt',
    #     control_prop=1.0)

    # Cell Reports reviewer rerun
    # filter_ids(
    #     param_folder='/home/upamanyu/GWANN/Code_AD/params/reviewer_rerun', 
    #     phen_cov_path='/mnt/sdg/UKB/Variables_UKB.txt')
    
    # dosage_percentage()
    # grp_size = 10
    # create_groups(
    #         label='MATERNAL_MARIONI',
    #         param_folder='/home/upamanyu/GWANN/Code_AD/params/reviewer_rerun_Sens2', 
    #         phen_cov_path='/mnt/sdg/UKB/Variables_UKB.txt',
    #         grp_size=grp_size, train_oversample=grp_size, test_oversample=grp_size
    #     )
    # num_wins_per_gene()
    # for chrom in range(2, 23, 2):
    #     print(f'Running chromosome: {chrom}')
    #     variant_gene_mapping(str(chrom), '/mnt/sdf/annovar_output')
    #     print()

    gene_PCs()